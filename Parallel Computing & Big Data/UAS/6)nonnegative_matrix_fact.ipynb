{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBV84tuQ-Nhn"
      },
      "source": [
        "<center> <b> open with a new tab </b> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48avETus-Nht"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nzhinusoftcm/review-on-collaborative-filtering/blob/master/6.NonNegativeMatrixFactorization.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f_2i9PS-Nhu"
      },
      "source": [
        "# Non-negative Matrix Factorization for Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2M46JFZ-Nhu"
      },
      "source": [
        "Jusl like Matrix Factorization (MF) [(Yehuda Koren et al., 2009)](https://ieeexplore.ieee.org/document/5197422), Non-negative Matrix Factorization (NMF in short) factors the rating matrix $R$ in two matrices in such a way that $R = PQ^{\\top}$.\n",
        "\n",
        "### One limitation of Matrix Factorization\n",
        "\n",
        "$P$ and $Q$ values in MF are non interpretable since their components can take arbitrary (positive and negative) values.\n",
        "\n",
        "### Particulariy of Non-negative Matrix Factorization\n",
        "\n",
        "NMF [(Lee and Seung, 1999)](http://www.dm.unibo.it/~simoncin/nmfconverge.pdf) allows the reconstruction of $P$ and $Q$ in such a way that $P,Q \\ge 0$. Constraining $P$ and $Q$ values to be taken from $[0,1]$ allows  a probabilistic interpretation\n",
        "\n",
        "- Latent factors represent groups of users who share the same tastes,\n",
        "- The value  $P_{u,l}$  represents the probability that user $u$ belongs to the group $l$ of users and \n",
        "- The value $Q_{l,i}$ represents the probability that users in the group $l$  likes item $i$.\n",
        "\n",
        "### Objective function\n",
        "\n",
        "With the Euclidian distance, the NMF objective function is defined by\n",
        "\n",
        "\\begin{equation}\n",
        "J = \\frac{1}{2}\\sum_{(u,i) \\in \\kappa}||R_{u,i} - P_uQ_i^{\\top}||^2 + \\lambda_P||P_u||^2 + \\lambda_Q||Q_i||^2\n",
        "\\end{equation}\n",
        "\n",
        "The goal is to minimize the cost function $J$ by optimizing parameters $P$ and $Q$, with $\\lambda_P$ and $\\lambda_Q$ the regularizer parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fy4xVXO-Nhw"
      },
      "source": [
        "### Multiplicative update rule\n",
        "\n",
        "According [(Lee and Seung, 1999)](https://www.nature.com/articles/44565.pdf), to the multiplicative update rule for $P$ and $Q$ are as follows :\n",
        "\n",
        "\\begin{equation}\n",
        "P \\leftarrow P \\cdot \\frac{RQ}{PQ^{\\top}Q}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "Q \\leftarrow Q \\cdot \\frac{R^{\\top}P}{QP^{\\top}P}\n",
        "\\end{equation}\n",
        "\n",
        "However, since $R$ is a sparse matrix, we need to update each $P_u$ according to existing ratings of user $u$. Similarily, we need to update $Q_i$ according to existing ratings on item $i$. Hence :\n",
        "\n",
        "\\begin{equation}\n",
        "P_{u,k} \\leftarrow P_{u,k} \\cdot \\frac{\\sum_{i \\in I_u}Q_{i,k}\\cdot r_{u,i}}{\\sum_{i \\in I_u}Q_{i,k}\\cdot \\hat{r}_{u,i} + \\lambda_P|I_u|P_{u,k}}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "Q_{i,k} \\leftarrow Q_{i,k} \\cdot \\frac{\\sum_{u \\in U_i}P_{u,k}\\cdot r_{u,i}}{\\sum_{u \\in U_i}P_{u,k}\\cdot \\hat{r}_{u,i} + \\lambda_Q|U_i|Q_{i,k}}\n",
        "\\end{equation}\n",
        "\n",
        "Where\n",
        "- $P_{u,k}$ is the $k^{th}$ latent factor of $P_u$\n",
        "- $Q_{i,k}$ is the $k^{th}$ latent factor of $Q_i$\n",
        "- $I_u$ the of items rated by user $u$\n",
        "- $U_i$ the set of users who rated item $i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "deTEZzLV-Nhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1ef012-8806-48c9-9f9b-96265cab35e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-03 11:00:55--  https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip [following]\n",
            "--2023-01-03 11:00:55--  https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15312323 (15M) [application/zip]\n",
            "Saving to: ‘recsys.zip’\n",
            "\n",
            "recsys.zip          100%[===================>]  14.60M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-01-03 11:00:56 (177 MB/s) - ‘recsys.zip’ saved [15312323/15312323]\n",
            "\n",
            "Archive:  recsys.zip\n",
            "   creating: recsys/\n",
            "  inflating: recsys/datasets.py      \n",
            "  inflating: recsys/preprocessing.py  \n",
            "  inflating: recsys/utils.py         \n",
            "  inflating: recsys/requirements.txt  \n",
            "   creating: recsys/.vscode/\n",
            "  inflating: recsys/.vscode/settings.json  \n",
            "   creating: recsys/__pycache__/\n",
            "  inflating: recsys/__pycache__/datasets.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/datasets.cpython-37.pyc  \n",
            "  inflating: recsys/__pycache__/utils.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-37.pyc  \n",
            "  inflating: recsys/__pycache__/datasets.cpython-38.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-38.pyc  \n",
            "   creating: recsys/memories/\n",
            "  inflating: recsys/memories/ItemToItem.py  \n",
            "  inflating: recsys/memories/UserToUser.py  \n",
            "   creating: recsys/memories/__pycache__/\n",
            "  inflating: recsys/memories/__pycache__/UserToUser.cpython-36.pyc  \n",
            "  inflating: recsys/memories/__pycache__/UserToUser.cpython-37.pyc  \n",
            "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-37.pyc  \n",
            "  inflating: recsys/memories/__pycache__/user2user.cpython-36.pyc  \n",
            "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-36.pyc  \n",
            "   creating: recsys/models/\n",
            "  inflating: recsys/models/SVD.py    \n",
            "  inflating: recsys/models/MatrixFactorization.py  \n",
            "  inflating: recsys/models/ExplainableMF.py  \n",
            "  inflating: recsys/models/NonnegativeMF.py  \n",
            "   creating: recsys/models/__pycache__/\n",
            "  inflating: recsys/models/__pycache__/SVD.cpython-36.pyc  \n",
            "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-37.pyc  \n",
            "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-36.pyc  \n",
            "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-37.pyc  \n",
            "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-36.pyc  \n",
            "   creating: recsys/metrics/\n",
            "  inflating: recsys/metrics/EvaluationMetrics.py  \n",
            "   creating: recsys/img/\n",
            "  inflating: recsys/img/MF-and-NNMF.png  \n",
            "  inflating: recsys/img/svd.png      \n",
            "  inflating: recsys/img/MF.png       \n",
            "   creating: recsys/predictions/\n",
            "   creating: recsys/predictions/item2item/\n",
            "   creating: recsys/weights/\n",
            "   creating: recsys/weights/item2item/\n",
            "   creating: recsys/weights/item2item/ml1m/\n",
            "  inflating: recsys/weights/item2item/ml1m/similarities.npy  \n",
            "  inflating: recsys/weights/item2item/ml1m/neighbors.npy  \n",
            "   creating: recsys/weights/item2item/ml100k/\n",
            "  inflating: recsys/weights/item2item/ml100k/similarities.npy  \n",
            "  inflating: recsys/weights/item2item/ml100k/neighbors.npy  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
        "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
        "    !unzip recsys.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKuAxXeL-Nhy"
      },
      "source": [
        "### Install and import useful packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj_zMyx8-Nhz"
      },
      "source": [
        "### requirements\n",
        "\n",
        "```\n",
        "matplotlib==3.2.2\n",
        "numpy==1.19.2\n",
        "pandas==1.0.5\n",
        "python==3.7\n",
        "scikit-learn==0.24.1\n",
        "scikit-surprise==1.1.1\n",
        "scipy==1.6.2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ok97810p-Nh0"
      },
      "outputs": [],
      "source": [
        "from recsys.preprocessing import mean_ratings\n",
        "from recsys.preprocessing import normalized_ratings\n",
        "from recsys.preprocessing import ids_encoder\n",
        "from recsys.preprocessing import train_test_split\n",
        "from recsys.preprocessing import rating_matrix\n",
        "from recsys.preprocessing import get_examples\n",
        "from recsys.preprocessing import scale_ratings\n",
        "\n",
        "from recsys.datasets import ml1m\n",
        "from recsys.datasets import ml100k\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jTcpYBM-Nh0"
      },
      "source": [
        "### Load and preprocess rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "id": "WKewoaFa-Nh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390bac87-a763-4ae8-9d70-1ac19533df86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download data 100.2%\n",
            "Successfully downloaded ml-100k.zip 4924029 bytes.\n",
            "Unzipping the ml-100k.zip zip file ...\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "# prepare data\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "# convert ratings from dataframe to numpy array\n",
        "np_ratings = ratings.to_numpy()\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column=\"rating\")\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n7XgLvS-Nh1"
      },
      "source": [
        "### Non-negative Matrix Factorization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2ccfSRTP-Nh1"
      },
      "outputs": [],
      "source": [
        "class NMF:\n",
        "    \n",
        "    def __init__(self, ratings, m, n, uencoder, iencoder, K=10, lambda_P=0.01, lambda_Q=0.01):\n",
        "        \n",
        "        np.random.seed(32)\n",
        "        \n",
        "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
        "        self.ratings = ratings\n",
        "        self.np_ratings = ratings.to_numpy()\n",
        "        self.K = K\n",
        "        self.P = np.random.rand(m, K)\n",
        "        self.Q = np.random.rand(n, K)\n",
        "        \n",
        "        # hyper parameter initialization\n",
        "        self.lambda_P = lambda_P\n",
        "        self.lambda_Q = lambda_Q\n",
        "\n",
        "        # initialize encoders\n",
        "        self.uencoder = uencoder\n",
        "        self.iencoder = iencoder\n",
        "        \n",
        "        # training history\n",
        "        self.history = {\n",
        "            \"epochs\": [],\n",
        "            \"loss\": [],\n",
        "            \"val_loss\": [],\n",
        "        }\n",
        "    \n",
        "    def print_training_parameters(self):\n",
        "        print('Training NMF ...')\n",
        "        print(f'k={self.K}')\n",
        "        \n",
        "    def mae(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        returns the Mean Absolute Error\n",
        "        \"\"\"\n",
        "        # number of training examples\n",
        "        m = x_train.shape[0]\n",
        "        error = 0\n",
        "        for pair, r in zip(x_train, y_train):\n",
        "            u, i = pair\n",
        "            error += abs(r - np.dot(self.P[u], self.Q[i]))\n",
        "        return error / m\n",
        "    \n",
        "    def update_rule(self, u, i, error):\n",
        "        I = self.np_ratings[self.np_ratings[:, 0] == u][:, [1, 2]]\n",
        "        U = self.np_ratings[self.np_ratings[:, 1] == i][:, [0, 2]]    \n",
        "                    \n",
        "        num = self.P[u] * np.dot(self.Q[I[:, 0]].T, I[:, 1])\n",
        "        dem = np.dot(self.Q[I[:, 0]].T, np.dot(self.P[u], self.Q[I[:, 0]].T)) + self.lambda_P * len(I) * self.P[u]\n",
        "        self.P[u] = num / dem\n",
        "\n",
        "        num = self.Q[i] * np.dot(self.P[U[:, 0]].T, U[:, 1])\n",
        "        dem = np.dot(self.P[U[:, 0]].T, np.dot(self.P[U[:, 0]], self.Q[i].T)) + self.lambda_Q * len(U) * self.Q[i]\n",
        "        self.Q[i] = num / dem\n",
        "    \n",
        "    @staticmethod\n",
        "    def print_training_progress(epoch, epochs, error, val_error, steps=5):\n",
        "        if epoch == 1 or epoch % steps == 0:\n",
        "            print(f\"epoch {epoch}/{epochs} - loss : {round(error, 3)} - val_loss : {round(val_error, 3)}\")\n",
        "                \n",
        "    def fit(self, x_train, y_train, validation_data, epochs=10):\n",
        "\n",
        "        self.print_training_parameters()\n",
        "        x_test, y_test = validation_data\n",
        "        for epoch in range(1, epochs+1):\n",
        "            for pair, r in zip(x_train, y_train):\n",
        "                u, i = pair\n",
        "                r_hat = np.dot(self.P[u], self.Q[i])\n",
        "                e = abs(r - r_hat)\n",
        "                self.update_rule(u, i, e)                \n",
        "            # training and validation error  after this epochs\n",
        "            error = self.mae(x_train, y_train)\n",
        "            val_error = self.mae(x_test, y_test)\n",
        "            self.update_history(epoch, error, val_error)\n",
        "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
        "        \n",
        "        return self.history\n",
        "    \n",
        "    def update_history(self, epoch, error, val_error):\n",
        "        self.history['epochs'].append(epoch)\n",
        "        self.history['loss'].append(error)\n",
        "        self.history['val_loss'].append(val_error)\n",
        "    \n",
        "    def evaluate(self, x_test, y_test):        \n",
        "        error = self.mae(x_test, y_test)\n",
        "        print(f\"validation error : {round(error,3)}\")\n",
        "        print('MAE : ', error)        \n",
        "        return error\n",
        "      \n",
        "    def predict(self, userid, itemid):\n",
        "        u = self.uencoder.transform([userid])[0]\n",
        "        i = self.iencoder.transform([itemid])[0]\n",
        "        r = np.dot(self.P[u], self.Q[i])\n",
        "        return r\n",
        "\n",
        "    def recommend(self, userid, N=30):\n",
        "        # encode the userid\n",
        "        u = self.uencoder.transform([userid])[0]\n",
        "\n",
        "        # predictions for users userid on all product\n",
        "        predictions = np.dot(self.P[u], self.Q.T)\n",
        "\n",
        "        # get the indices of the top N predictions\n",
        "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
        "\n",
        "        # decode indices to get their corresponding itemids\n",
        "        top_items = self.iencoder.inverse_transform(top_idx)\n",
        "\n",
        "        # take corresponding predictions for top N indices\n",
        "        preds = predictions[top_idx]\n",
        "\n",
        "        return top_items, preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj_uaPNu-Nh2"
      },
      "source": [
        "### Train the NMF model with ML-100K dataset\n",
        "\n",
        "model parameters :\n",
        "\n",
        "- $k = 10$ : (number of factors)\n",
        "- $\\lambda_P = 0.6$\n",
        "- $\\lambda_Q = 0.6$\n",
        "- epochs = 10\n",
        "\n",
        "Note that it may take some time to complete the training on 10 epochs (around 7 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "40EZjDaS-Nh3",
        "outputId": "b47ad4d8-eee7-4688-c1e5-58903e268906",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training NMF ...\n",
            "k=10\n",
            "epoch 1/10 - loss : 0.916 - val_loss : 0.917\n",
            "epoch 2/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 3/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 4/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 5/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 6/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 7/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 8/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 9/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 10/10 - loss : 0.915 - val_loss : 0.917\n"
          ]
        }
      ],
      "source": [
        "m = ratings['userid'].nunique()   # total number of users\n",
        "n = ratings['itemid'].nunique()   # total number of items\n",
        "\n",
        "# create and train the model\n",
        "nmf = NMF(ratings, m, n, uencoder, iencoder, K=10, lambda_P=0.6, lambda_Q=0.6)\n",
        "history = nmf.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S_MQMSLg-Nh5",
        "outputId": "4876245e-77fa-49ed-a789-ad2449105af8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation error : 0.917\n",
            "MAE :  0.9165041343019539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9165041343019539"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nmf.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwQ5peR4-Nh6"
      },
      "source": [
        "## Evaluation of NMF with Scikit-suprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcaO3ax--Nh6"
      },
      "source": [
        "We can use the [scikt-suprise](https://surprise.readthedocs.io/en/stable/) package to train the NMF model. It  is an easy-to-use Python scikit for recommender systems.\n",
        "\n",
        "1. Import the NMF class from the suprise scikit.\n",
        "2. Load the data with the built-in function\n",
        "3. Instanciate NMF with ```k=10``` (```n_factors```) and we use 10 epochs (```n_epochs```)\n",
        "4. Evaluate the model using cross-validation with 5 folds."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TPFYB-PZ0nq",
        "outputId": "49d5732c-7bfe-4f60-dc32-cd1bcc807e82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[K     |████████████████████████████████| 771 kB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.7.3)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp38-cp38-linux_x86_64.whl size=2626458 sha256=4af53178cbc7b80ea173755274c23dfbae0129b2f02f6be7acaa009033859425\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/db/86/2c18183a80ba05da35bf0fb7417aac5cddbd93bcb1b92fd3ea\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-0RryI6G-Nh9",
        "outputId": "f7b37d1f-240f-4c5d-baf2-c0a88dd44610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n",
            "Evaluating MAE of algorithm NMF on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "MAE (testset)     0.9626  0.9435  0.9777  0.9607  0.9510  0.9591  0.0116  \n",
            "Fit time          0.41    0.44    0.44    0.45    0.43    0.43    0.01    \n",
            "Test time         0.21    0.34    0.14    0.22    0.16    0.21    0.07    \n"
          ]
        }
      ],
      "source": [
        "from surprise import NMF\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "# Load the movielens-100k dataset (download it if needed).\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Use the NMF algorithm.\n",
        "nmf = NMF(n_factors=10, n_epochs=10)\n",
        "\n",
        "# Run 5-fold cross-validation and print results.\n",
        "history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POgmyE5P-Nh-"
      },
      "source": [
        "As result, the mean MAE on the test set is **mae = 0.9510** which is equivalent to the result we have obtained on *ml-100k* with our own implementation **mae = 0.9165**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRmTJ3nH-Nh-"
      },
      "source": [
        "#### ML-1M\n",
        "\n",
        "The mean MAE on a 5-fold cross-validation is **mae = 0.9567**\n",
        "\n",
        "This may take around 2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wWUDSZiT-Nh-",
        "outputId": "412c25e3-e98c-4fe2-c2fa-6e9711d64364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-1m could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-1m.zip...\n",
            "Done! Dataset ml-1m has been saved to /root/.surprise_data/ml-1m\n",
            "Evaluating MAE of algorithm NMF on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "MAE (testset)     0.9559  0.9580  0.9566  0.9639  0.9490  0.9567  0.0047  \n",
            "Fit time          3.76    4.31    4.36    4.38    4.35    4.23    0.24    \n",
            "Test time         3.39    2.54    2.26    2.94    2.33    2.69    0.42    \n"
          ]
        }
      ],
      "source": [
        "data = Dataset.load_builtin('ml-1m')\n",
        "nmf = NMF(n_factors=10, n_epochs=10)\n",
        "history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1b3UhV-Nh_"
      },
      "source": [
        "## Explainable Matrix Factorization\n",
        "\n",
        "NMF introduced explainability to MF by constraining $P$ and $Q$ values in $[0,1]$. It can be considered is an inner explainable model. It's possible to inject external informations into the model in order to bring explainability. This is what the **Explainable Matrix Factorization (EMF)** algorithm does. It computes explainable scores from user or item similarities taken from the user-based or item-based CF.\n",
        "\n",
        "Click [here](https://github.com/nzhinusoftcm/review-on-collaborative-filtering/blob/master/7.Explainable_Matrix_Factorization.ipynb) to open the EMF notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUiir4RH-Nh_"
      },
      "source": [
        "## Reference\n",
        "\n",
        "1. Daniel D. Lee & H. Sebastian Seung (1999). [Learning the parts of objects by non-negative matrix factorization](https://www.nature.com/articles/44565)\n",
        "2. Deng Cai et al. (2008). [Non-negative Matrix Factorization on Manifold](https://ieeexplore.ieee.org/document/4781101)\n",
        "3. Yu-Xiong Wang and Yu-Jin Zhang (2011). [Non-negative Matrix Factorization: a Comprehensive Review](https://ieeexplore.ieee.org/document/6165290)\n",
        "4. Nicolas Gillis (2014). [The Why and How of Nonnegative Matrix Factorization](https://arxiv.org/pdf/1401.5226.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_ER4ZU8-NiA"
      },
      "source": [
        "## Author\n",
        "\n",
        "[Carmel WENGA](https://www.linkedin.com/in/carmel-wenga-871876178/), <br>\n",
        "PhD student at Université de la Polynésie Française, <br> \n",
        "Applied Machine Learning Research Engineer, <br>\n",
        "[ShoppingList](https://shoppinglist.cm), NzhinuSoft."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
